# SumoLogic to CloudZero Adapter

A serverless adapter that extracts billing data from SumoLogic and streams it to CloudZero. This adapter processes usage data for logs, metrics, traces, and storage, converting them to CloudZero's billing format and uploading the via API.

## Overview

The adapter queries SumoLogic for usage data across different tiers and services:
- **Logs**: Continuous, Frequent, and Infrequent tiers (ingestion and scanning)
- **Storage**: Log storage and infrequent log storage (daily usage)
- **Metrics**: Datapoint ingestion
- **Traces**: Span ingestion

Data is converted to CloudZero Billing Format (CBF) and streamed to the CloudZero Anycost API.


## Environment Variables

### Required Variables

**SumoLogic Configuration:**
- `SUMO_ACCESS_KEY`: Your SumoLogic access key
- `SUMO_SECRET_KEY`: Your SumoLogic secret key
- `SUMO_DEPLOYMENT`: SumoLogic deployment (e.g., "us1", "us2", "eu", "au")
- `SUMO_ORG_ID`: Your SumoLogic organization ID

**CloudZero Configuration:**
- `CZ_AUTH_KEY`: CloudZero API authentication key
- `CZ_ANYCOST_STREAM_CONNECTION_ID`: CloudZero stream connection ID

### Optional Variables

For best results lookup your credit rates and cost per credit from your SumoLogic contract and set them using
these env variables. The defaults are not guaranteed to be correct for your contract. 

**Credit Rates (defaults shown):**
- `LOG_CONTINUOUS_CREDIT_RATE`: 20
- `LOG_FREQUENT_CREDIT_RATE`: 9
- `LOG_INFREQUENT_CREDIT_RATE`: 0.4
- `LOG_INFREQUENT_SCAN_CREDIT_RATE`: 0.016
- `METRICS_CREDIT_RATE`: 3
- `TRACING_CREDIT_RATE`: 14
- `COST_PER_CREDIT`: 0.15

**Backfill Options:**
- `BACKFILL_MODE`: Enable backfill mode (set automatically by command line)
- `BACKFILL_START_DATE`: Start date for backfill (YYYY-MM-DD format)
- `BACKFILL_END_DATE`: End date for backfill (YYYY-MM-DD format)
- `DRY_RUN_MODE`: Preview mode without uploading data (set by --dry-run)
- `RESUME_MODE`: Enable resume functionality (set automatically)
- `AUTO_RESUME`: Enable automatic resume from state file

**Other Options:**
- `CZ_URL`: CloudZero API endpoint (default: "https://api.cloudzero.com")
- `QUERY_TIME_DAYS`: Days of historical data to query in standard mode (default: 1)
- `LOGGING_LEVEL`: Log level - "INFO" or "DEBUG" (default: "INFO")

## Usage Modes

### Standard Mode (Daily Operation)
Processes the previous 24 hours of data - designed for daily scheduled execution:
```bash
./test_execute.sh                              # Process last 24 hours
```

### Backfill Mode (Historical Data)
Process historical data for date ranges or specific periods. Includes automatic chunking for large datasets and intelligent resume capabilities:

```bash
# Backfill specific date range
./test_execute.sh --backfill-start 2024-01-01 --backfill-end 2024-01-31

# Backfill last N days
./test_execute.sh --days 30

# Preview backfill without uploading (dry-run)
./test_execute.sh --days 7 --dry-run
```

### Automatic Resume System
The adapter includes a robust automatic resume system for handling interrupted backfills:

```bash
# Start a backfill
./test_execute.sh --days 30

# If interrupted, automatically resume from where it left off
./test_execute.sh --resume                     # No date needed - automatic!

# Manual resume (if needed)
./test_execute.sh --resume 2024-01-15          # Resume from specific date
```

**How it works:**
- Creates state files (`.backfill_state_*.json`) to track progress
- Only marks days as complete when ALL services upload successfully
- Automatically resumes from the day after the last successful day
- Cleans up state files when backfill completes
- Safe for dry-run mode (doesn't create state files)

### Advanced Options

```bash
# Verbose logging for troubleshooting
./test_execute.sh --days 7 --verbose

# Quiet mode for production
./test_execute.sh --quiet

# Combine options
./test_execute.sh --days 30 --dry-run --verbose
```

## Local Development

### Prerequisites
- Python 3.13 (or compatible version)
- [UV package manager](https://docs.astral.sh/uv/) (recommended) or pip in a pyenv
- Docker (optional for container builds)

### Setup

1. **Install dependencies:**
```bash
# Using UV (recommended)
uv sync --group dev

# Or using pip
pip install -r requirements.txt
```

2. **Configure environment variables:**

Edit test_execute.sh with your actual credentials

### Execute Locally

```bash
# Standard daily operation
./test_execute.sh

# Historical backfill
./test_execute.sh --days 7 --dry-run
```


## Project Structure

```
├── sumo_anycost_lambda.py      # Main application code
├── requirements.txt            # Python dependencies (auto-generated by uv)
├── pyproject.toml             # Project configuration and dependencies
├── uv.lock                    # Lock file for reproducible builds
├── Dockerfile                 # Container image for Lambda deployment
├── create_lambda_zip.sh       # Automated Lambda zip creation script
├── test_execute.sh            # Template test script
└── README.md                  # This file
```

### Key Files

- **`sumo_anycost_lambda.py`**: Single-file application containing all logic
- **`create_lambda_zip.sh`**: Automated script to create optimized Lambda deployment packages
- **`Dockerfile`**: Production-ready container configuration for AWS Lambda
- **`pyproject.toml`**: UV config file; uses poethepoet for development task automation

## AWS Lambda Deployment

### Method 1: Using Automated ZIP Creation (Recommended)

1. **Create deployment package using the provided script:**
```bash
# The script handles everything automatically
./create_lambda_zip.sh
```

This creates `sumo-anycost-lambda.zip` (optimized for Lambda) containing:
- All dependencies from `requirements.txt` with Linux compatibility
- Main Lambda function code (renamed to `lambda_function.py`)
- Size-optimized (removes unnecessary files)
- Validates size limits


2. **Deploy with AWS CLI:**
```bash
# Create the function
aws lambda create-function \
  --function-name sumo-cz-adapter \
  --runtime python3.13 \
  --role arn:aws:iam::YOUR-ACCOUNT:role/lambda-execution-role \
  --handler lambda_function.lambda_handler \
  --zip-file fileb://sumo-anycost-lambda.zip \
  --timeout 900

# Set environment variables
aws lambda update-function-configuration \
  --function-name sumo-cz-adapter \
  --environment Variables='{
    "SUMO_ACCESS_KEY":"your-key",
    "SUMO_SECRET_KEY":"your-secret",
    "SUMO_DEPLOYMENT":"us1",
    "SUMO_ORG_ID":"your-org-id",
    "CZ_AUTH_KEY":"your-cz-key",
    "CZ_ANYCOST_STREAM_CONNECTION_ID":"your-stream-id"
  }'
```

Note: You can also deploy the zip file through the AWS Lambda UI. Don't forget to set the environmental variables 
(see test_execute.sh for all available variables.) It is recommended to set your AWS Lambda function timeout to
15 minutes (900 seconds) as this script can take a while to complete. 

### Method 2: Using Container Image

The repository includes a production-ready `Dockerfile` using AWS Lambda Python 3.13 base image.

1. **Build the Docker image:**
```bash
# Build image using provided Dockerfile
docker build -t sumo-cz-adapter .
```

2. **Push to Amazon ECR and deploy:**
```bash
# Create ECR repository
aws ecr create-repository --repository-name sumo-cz-adapter

# Get ECR login token
aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin YOUR-ACCOUNT.dkr.ecr.us-east-1.amazonaws.com

# Tag and push image
docker tag sumo-cz-adapter:latest YOUR-ACCOUNT.dkr.ecr.us-east-1.amazonaws.com/sumo-cz-adapter:latest
docker push YOUR-ACCOUNT.dkr.ecr.us-east-1.amazonaws.com/sumo-cz-adapter:latest

# Create Lambda function using container image
aws lambda create-function \
  --function-name sumo-cz-adapter \
  --package-type Image \
  --code ImageUri=YOUR-ACCOUNT.dkr.ecr.us-east-1.amazonaws.com/sumo-cz-adapter:latest \
  --role arn:aws:iam::YOUR-ACCOUNT:role/lambda-execution-role \
  --timeout 300 \
  --environment Variables='{
    "SUMO_ACCESS_KEY":"your-key",
    "SUMO_SECRET_KEY":"your-secret",
    "SUMO_DEPLOYMENT":"us1",
    "SUMO_ORG_ID":"your-org-id",
    "CZ_AUTH_KEY":"your-cz-key",
    "CZ_ANYCOST_STREAM_CONNECTION_ID":"your-stream-id"
  }'
```

### Scheduling

**IMPORTANT:** This script must be run exactly once every 24 hours. Running it at any other frequency will cause incorrect data reporting.

Set up EventBridge to run daily:
```bash
aws events put-rule \
  --name sumo-cz-adapter-daily \
  --schedule-expression "rate(24 hours)"

aws lambda add-permission \
  --function-name sumo-cz-adapter \
  --statement-id allow-eventbridge \
  --action lambda:InvokeFunction \
  --principal events.amazonaws.com \
  --source-arn arn:aws:events:us-east-1:YOUR-ACCOUNT:rule/sumo-cz-adapter-daily
```


## Monitoring and Troubleshooting

### Logging
- Set `LOGGING_LEVEL=DEBUG` for detailed logging output
- All API calls are logged with debug information
- Storage filtering shows which date is being processed

### Common Issues

**Function Timeout:**
- Standard mode: Minimum timeout 300 seconds (5 minutes)
- Backfill mode: May require 900+ seconds for large date ranges
- SumoLogic export API can take time to generate reports
- Consider increasing timeout for large data volumes

**Rate Limiting:**
- Adapter includes exponential backoff for 429 errors
- Monitor CloudWatch logs for rate limiting patterns
- SumoLogic API has rate limits per minute
- Backfill mode processes day-by-day to manage rate limits

**Backfill Issues:**
- Large datasets automatically chunked to stay under 10MB API limit
- State files track progress for automatic resume capability
- Use `--dry-run` to preview backfill without uploading data
- Failed services on any day prevent that day from being marked complete

**Resume Functionality:**
- State files created as `.backfill_state_YYYYMMDD_to_YYYYMMDD.json`
- Only days with ALL successful services are marked complete
- Use `--resume` (no date) for automatic resume from state file
- State files cleaned up automatically when backfill completes

**Date Parsing Errors:**
- Fixed in current version with flexible date parsing
- Handles both MM/dd/yy and ISO date formats from SumoLogic

**Empty Results:**
- Storage filtering may return no data if previous day has no usage
- Verify SumoLogic organization has data for the date range
- Check SUMO_ORG_ID matches your organization

**CloudZero Stream Errors:**
- Verify CZ_ANYCOST_STREAM_CONNECTION_ID is correct
- Check CloudZero API authentication
- Monitor for 4xx/5xx responses in logs
- Large payloads automatically chunked to prevent upload failures

### Testing
```bash
# Test standard mode with debug logging
export LOGGING_LEVEL=DEBUG
./test_execute.sh

# Test backfill with dry-run (safe testing)
./test_execute.sh --days 7 --dry-run --verbose

# Test automatic resume functionality
./test_execute.sh --days 3 --dry-run    # Start backfill
./test_execute.sh --resume              # Test auto-resume

# Check state file creation (after non-dry-run backfill)
ls -la .backfill_state_*.json

# Check zip file contents
unzip -l sumo-anycost-lambda.zip

# Validate Docker build
docker build -t test-adapter . && docker images test-adapter
```

## Security Considerations

- Store API keys and secrets in your cloud provider's secret management service
- Use IAM roles with minimal required permissions
- Enable function logging and monitoring
- Consider VPC deployment for enhanced security